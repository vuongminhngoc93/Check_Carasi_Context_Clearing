import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import os

# Define paths relative to current script location
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(script_dir))
logs_dir = os.path.join(project_root, 'Modules', 'Logs')

# Try to find performance data file
performance_files = [
    'PerformanceAnalysis.csv',
    'PerformanceAnalysis_OPTIMIZED.csv', 
    'PerformanceAnalysis_OPTIMIZED_V2.csv'
]

df = None
used_file = None

for file in performance_files:
    file_path = os.path.join(logs_dir, file)
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path)
            used_file = file
            break
        except Exception as e:
            print(f"âš ï¸ Error reading {file}: {e}")
            continue

if df is None:
    print("âŒ No valid performance data files found in Modules/Logs/")
    print(f"ðŸ“ Checked directory: {logs_dir}")
    print("ðŸ” Available files:")
    if os.path.exists(logs_dir):
        for file in os.listdir(logs_dir):
            if file.endswith('.csv'):
                print(f"   ðŸ“„ {file}")
    exit(1)

print(f"ðŸ“Š Using performance data from: {used_file}")
print("=== ðŸ“Š PERFORMANCE ANALYSIS REPORT ===\n")

# Basic statistics
total_operations = len(df)
unique_operations = df['OperationName'].nunique()
test_duration = pd.to_datetime(df['Timestamp'].iloc[-1]) - pd.to_datetime(df['Timestamp'].iloc[0])

print(f"ðŸ“ˆ OVERVIEW:")
print(f"  â€¢ Total logged operations: {total_operations}")
print(f"  â€¢ Unique operation types: {unique_operations}")
print(f"  â€¢ Test duration: {test_duration}")
print(f"  â€¢ Log file size: 59KB\n")

# Performance by operation type
print("ðŸ”§ PERFORMANCE BY OPERATION TYPE:")
operation_stats = df.groupby('OperationName').agg({
    'ElapsedMs': ['count', 'mean', 'max', 'min', 'std'],
    'MemoryMB': ['mean', 'max']
}).round(2)

operation_stats.columns = ['Count', 'Avg_Time', 'Max_Time', 'Min_Time', 'Std_Time', 'Avg_Memory', 'Max_Memory']

# Sort by average time descending
operation_stats = operation_stats.sort_values('Avg_Time', ascending=False)

for operation, stats in operation_stats.iterrows():
    print(f"  ðŸ”¹ {operation}:")
    print(f"     Count: {stats['Count']}, Avg: {stats['Avg_Time']}ms, Max: {stats['Max_Time']}ms")
    
    # Identify performance issues
    if stats['Avg_Time'] > 1000:
        print(f"     âš ï¸  SLOW: Average time > 1000ms")
    if stats['Max_Time'] > 2000:
        print(f"     ðŸš¨ CRITICAL: Max time > 2000ms")
    
    print()

# Memory analysis
print("ðŸ’¾ MEMORY ANALYSIS:")
print(f"  â€¢ Memory range: {df['MemoryMB'].min():.1f}MB - {df['MemoryMB'].max():.1f}MB")
print(f"  â€¢ Memory growth: {df['MemoryMB'].max() - df['MemoryMB'].min():.1f}MB total")
print(f"  â€¢ Peak memory usage: {df['MemoryMB'].max():.1f}MB")

# Tab count analysis
print(f"\nðŸ—‚ï¸ TAB ANALYSIS:")
print(f"  â€¢ Tab range: {df['TabCount'].min()} - {df['TabCount'].max()} tabs")
print(f"  â€¢ Final tab count: {df['TabCount'].iloc[-1]} tabs")

# Search operation analysis
search_ops = df[df['OperationName'] == 'Search_Operation']['ElapsedMs']
if len(search_ops) > 0:
    print(f"\nðŸ” SEARCH PERFORMANCE:")
    print(f"  â€¢ Total searches: {len(search_ops)}")
    print(f"  â€¢ Average search time: {search_ops.mean():.0f}ms")
    print(f"  â€¢ Slowest search: {search_ops.max():.0f}ms")
    print(f"  â€¢ Fastest search: {search_ops.min():.0f}ms")

# Tab creation analysis
tab_create_ops = df[df['OperationName'] == 'Create_New_Tab']['ElapsedMs']
if len(tab_create_ops) > 0:
    print(f"\nðŸ“‘ TAB CREATION PERFORMANCE:")
    print(f"  â€¢ Total tab creations: {len(tab_create_ops)}")
    print(f"  â€¢ Average creation time: {tab_create_ops.mean():.0f}ms")
    print(f"  â€¢ Slowest creation: {tab_create_ops.max():.0f}ms")
    
    # Count slow tab creations
    slow_tabs = tab_create_ops[tab_create_ops > 100]
    print(f"  â€¢ Slow tab creations (>100ms): {len(slow_tabs)}")

# Performance trends
print(f"\nðŸ“Š PERFORMANCE TRENDS:")

# Search performance vs tab count
search_complete = df[df['OperationName'] == 'Search_Operation']
if len(search_complete) > 0:
    correlation = search_complete['ElapsedMs'].corr(search_complete['TabCount'])
    print(f"  â€¢ Search time vs Tab count correlation: {correlation:.3f}")
    
    if correlation > 0.5:
        print(f"    âš ï¸  Strong positive correlation - performance degrades with more tabs")
    elif correlation > 0.3:
        print(f"    âš ï¸  Moderate correlation - tabs impact performance")

# Memory growth pattern
memory_growth_rate = (df['MemoryMB'].iloc[-1] - df['MemoryMB'].iloc[0]) / len(df) * 100
print(f"  â€¢ Memory growth rate: {memory_growth_rate:.2f}MB per 100 operations")

if memory_growth_rate > 1:
    print(f"    ðŸš¨ HIGH memory growth - potential memory leak")

print(f"\n=== ðŸŽ¯ OPTIMIZATION RECOMMENDATIONS ===")
print()

# Specific recommendations based on data
recommendations = []

# Check search performance
if search_ops.mean() > 1500:
    recommendations.append("ðŸ”§ SEARCH OPTIMIZATION: Average search time > 1.5s - optimize Excel parsing")

# Check tab creation
if tab_create_ops.mean() > 400:
    recommendations.append("ðŸ“‘ TAB OPTIMIZATION: Tab creation > 400ms - optimize UC creation")

# Check memory usage
if df['MemoryMB'].max() > 60:
    recommendations.append("ðŸ’¾ MEMORY OPTIMIZATION: Peak usage > 60MB - implement better cleanup")

# Check performance degradation
if correlation > 0.5:
    recommendations.append("ðŸ“Š SCALING OPTIMIZATION: Performance degrades with tab count - implement tab virtualization")

if not recommendations:
    recommendations.append("âœ… GOOD PERFORMANCE: No critical issues detected")

for i, rec in enumerate(recommendations, 1):
    print(f"{i}. {rec}")

print(f"\n=== ðŸ“‹ SUMMARY ===")
print(f"Performance data shows {len(search_ops)} searches with average {search_ops.mean():.0f}ms")
print(f"Memory usage: {df['MemoryMB'].min():.1f}MB â†’ {df['MemoryMB'].max():.1f}MB")
print(f"Tab scaling: 1 â†’ {df['TabCount'].max()} tabs")

# Overall performance grade
avg_search_time = search_ops.mean() if len(search_ops) > 0 else 0
memory_usage = df['MemoryMB'].max()

if avg_search_time < 1000 and memory_usage < 50:
    grade = "A - Excellent"
elif avg_search_time < 1500 and memory_usage < 60:
    grade = "B - Good"
elif avg_search_time < 2000 and memory_usage < 70:
    grade = "C - Fair"
else:
    grade = "D - Needs Optimization"

print(f"Overall Performance Grade: {grade}")
